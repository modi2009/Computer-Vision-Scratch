{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10827165,"sourceType":"datasetVersion","datasetId":6723075},{"sourceId":6088,"sourceType":"modelInstanceVersion","modelInstanceId":4626,"modelId":2800}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Filesystem & data handling\nimport os\nimport glob\nimport pathlib\n\nimport numpy as np\nimport pandas as pd\n\n# Image processing\nimport cv2\n\n# TensorFlow (for TFRecord creation & later data pipeline)\nimport tensorflow as tf\n\n# Visualization (for debugging bounding boxes & annotations)\nimport matplotlib.pyplot as plt\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_dataset_distribution(annotations_dir):\n    \"\"\"\n    Analyzes the distribution of bounding boxes across a dataset.\n    It returns the maximum number of boxes found and visualizes the\n    distribution with a histogram.\n\n    Args:\n        annotations_dir (str): Path to the directory containing annotation files (.txt).\n    \n    Returns:\n        int: The maximum number of bounding boxes found in a single file.\n    \"\"\"\n    if not os.path.isdir(annotations_dir):\n        print(f\"Error: Directory not found at {annotations_dir}\")\n        return 0\n\n    box_counts = []\n    max_boxes = 0\n\n    print(\"Analyzing dataset...\")\n    # Iterate through all files in the directory\n    for filename in os.listdir(annotations_dir):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(annotations_dir, filename)\n            try:\n                # Count the number of lines in the file\n                with open(filepath, 'r') as f:\n                    num_lines = sum(1 for line in f)\n                \n                box_counts.append(num_lines)\n                \n                # Update the maximum count if necessary\n                if num_lines > max_boxes:\n                    max_boxes = num_lines\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}. Skipping. Error: {e}\")\n    \n    if not box_counts:\n        print(\"No annotation files found or all files were empty.\")\n        return 0\n    \n    print(f\"Analysis complete. Found {len(box_counts)} annotation files.\")\n\n    # Visualize the distribution with a histogram\n    plt.figure(figsize=(10, 6))\n    plt.hist(box_counts, bins=range(0, max_boxes + 2), edgecolor='black', alpha=0.7)\n    plt.axvline(max_boxes, color='red', linestyle='dashed', linewidth=2, label=f'Max Boxes: {max_boxes}')\n    plt.title('Distribution of Bounding Boxes per Image')\n    plt.xlabel('Number of Bounding Boxes')\n    plt.ylabel('Number of Images')\n    plt.xticks(np.arange(0, max_boxes + 2, step=1))\n    plt.grid(axis='y', linestyle='--', alpha=0.6)\n    plt.legend()\n    plt.tight_layout()\n    \n    # Save the plot\n    plot_filename = 'box_distribution.png'\n    plt.savefig(plot_filename)\n    print(f\"Distribution plot saved as '{plot_filename}'\")\n    \n    return max_boxes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_boxes_found = analyze_dataset_distribution('/kaggle/input/rdd-2022/RDD_SPLIT/train/labels')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_image_and_labels(image_path, annot_path):\n    \"\"\"\n    Reads image and annotation, prepares data.\n    This version uses only TensorFlow for file I/O to avoid\n    the dependency on the cv2 library.\n\n    Args:\n        image_path (str): Path to the image file.\n        annot_path (str): Path to the annotation file.\n        \n    Returns:\n        dict with image metadata and annotations, or None if an error occurs.\n    \"\"\"\n    try:\n        # Read the image as compressed bytes\n        image_bytes = tf.io.read_file(image_path)\n        \n        # Decode the image to get its shape.\n        # The image is immediately cast back to bytes to save memory.\n        image_decoded = tf.io.decode_image(image_bytes, channels=3)\n        height, width, _ = image_decoded.shape\n        \n        if height is None or width is None:\n            raise ValueError(\"Could not decode image to get shape.\")\n\n    except tf.errors.OpError as e:\n        print(f\"Warning: Could not read or decode image {image_path}. Skipping. Error: {e}\")\n        return None\n    except ValueError as e:\n        print(f\"Warning: {e} for image {image_path}. Skipping.\")\n        return None\n\n    # --- Read annotations ---\n    bboxes = []\n    labels = []\n    \n    if os.path.exists(annot_path) and os.path.getsize(annot_path) > 0:\n        with open(annot_path, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) != 5:\n                    continue\n                    \n                try:\n                    class_id = int(parts[0])\n                    x_center, y_center, box_w, box_h = map(float, parts[1:])\n                except ValueError:\n                    print(f\"Warning: Skipping malformed line in {annot_path}: '{line.strip()}'\")\n                    continue\n                \n                # Convert YOLO format to normalized (xmin, ymin, xmax, ymax)\n                xmin = x_center - (box_w / 2)\n                ymin = y_center - (box_h / 2)\n                xmax = x_center + (box_w / 2)\n                ymax = y_center + (box_h / 2)\n                \n                # Clip coordinates to the [0, 1] range\n                xmin, ymin = max(0.0, xmin), max(0.0, ymin)\n                xmax, ymax = min(1.0, xmax), min(1.0, ymax)\n                \n                bboxes.append([xmin, ymin, xmax, ymax])\n                labels.append(class_id)\n                \n    return {\n        \"filename\": os.path.basename(image_path),\n        \"height\": height,\n        \"width\": width,\n        \"image_bytes\": image_bytes,\n        \"bboxes\": bboxes,\n        \"labels\": labels\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.patches as patches\ndef visualize_parsed_record(record, class_names=None):\n    \"\"\"\n    Visualize one parsed record (image + bounding boxes).\n    \n    Args:\n        record (dict): Output from parse_image_and_labels()\n        class_names (list or dict): Optional mapping {id: name} for labels\n    \"\"\"\n    # Decode image bytes back to array\n    # Read the image as compressed bytes\n    image = tf.image.decode_jpeg(record['image_bytes'], channels=3)  # Specifically for JPEG\n    image_array = image.numpy()\n\n    height, width = record[\"height\"], record[\"width\"]\n    print(record[\"bboxes\"])\n    fig, ax = plt.subplots(1, figsize=(8, 6))\n    ax.imshow(image_array)\n    ax.set_title(record[\"filename\"])\n    \n    # Draw bounding boxes\n    for i, (xmin, ymin, xmax, ymax) in enumerate(record[\"bboxes\"]):\n        # Convert from normalized to absolute pixel coords\n        xmin_px, ymin_px = int(xmin * width), int(ymin * height)\n        xmax_px, ymax_px = int(xmax * width), int(ymax * height)\n\n        rect = patches.Rectangle(\n            (xmin_px, ymin_px),\n            xmax_px - xmin_px,\n            ymax_px - ymin_px,\n            linewidth=2,\n            edgecolor=\"red\",\n            facecolor=\"none\"\n        )\n        ax.add_patch(rect)\n\n        # Add label\n        if len(record[\"labels\"]) > i:\n            class_id = record[\"labels\"][i]\n            label = class_names[class_id] if class_names else str(class_id)\n            ax.text(\n                xmin_px, ymin_px - 5, label,\n                color=\"yellow\", fontsize=10,\n                bbox=dict(facecolor=\"red\", alpha=0.5, edgecolor=\"none\")\n            )\n\n    plt.axis(\"off\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Pick one image + annotation\nimg_path = \"/kaggle/input/rdd-2022/RDD_SPLIT/train/images/China_Drone_000001.jpg\"\nannot_path = \"/kaggle/input/rdd-2022/RDD_SPLIT/train/labels/China_Drone_000001.txt\"\nrecord = parse_image_and_labels(img_path, annot_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_parsed_record(record)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _bytes_feature(value):\n    \"\"\"Return a bytes_list from a string/byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):  # EagerTensor check\n        value = value.numpy()\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n    \"\"\"Return a float_list from a list of floats.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _int64_feature(value):\n    \"\"\"Return an int64_list from a list of ints.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_tf_example(record):\n    \"\"\"\n    Convert parsed image/annotation dict into a tf.train.Example.\n\n    Args:\n        record (dict): Output from parse_image_and_labels()\n\n    Returns:\n        tf.train.Example\n    \"\"\"\n    filename    = record[\"filename\"]\n    height      = record[\"height\"]\n    width       = record[\"width\"]\n    image_bytes = record[\"image_bytes\"]\n    bboxes      = record[\"bboxes\"]   # list of [xmin, ymin, xmax, ymax] normalized\n    labels      = record[\"labels\"]   # list of int class IDs\n\n    # Split bboxes into separate lists (stay empty if no objects)\n    xmins, ymins, xmaxs, ymaxs = [], [], [], []\n    for (xmin, ymin, xmax, ymax) in bboxes:\n        xmins.append(xmin)\n        ymins.append(ymin)\n        xmaxs.append(xmax)\n        ymaxs.append(ymax)\n\n    # Build feature dict\n    feature_dict = {\n        \"image/height\": _int64_feature([height]),\n        \"image/width\": _int64_feature([width]),\n        \"image/filename\": _bytes_feature(filename.encode(\"utf8\")),\n        \"image/encoded\": _bytes_feature(image_bytes),\n        \"image/object/bbox/xmin\": _float_feature(xmins),\n        \"image/object/bbox/ymin\": _float_feature(ymins),\n        \"image/object/bbox/xmax\": _float_feature(xmaxs),\n        \"image/object/bbox/ymax\": _float_feature(ymaxs),\n        \"image/object/class/label\": _int64_feature(labels),\n    }\n\n    return tf.train.Example(features=tf.train.Features(feature=feature_dict))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\ndef write_tfrecord(image_dir: str, annot_dir: str, output_dir: str, num_shards: int):\n    \"\"\"\n    Creates TFRecord shards from image and annotation directories.\n\n    Args:\n        image_dir (str): Path to the images folder.\n        annot_dir (str): Path to the annotations folder.\n        output_dir (str): Path where the .tfrecord files will be saved.\n        num_shards (int): The total number of TFRecord shards to create.\n    \"\"\"\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Get all image paths and sort them to ensure consistent sharding\n    all_img_paths = sorted(glob.glob(os.path.join(image_dir, \"*.jpg\")))\n    total_images = len(all_img_paths)\n    print(total_images)\n    \n    # Calculate the number of images per shard\n    shard_size = math.ceil(total_images / num_shards)\n    print(shard_size)\n    \n    print(f\"Starting TFRecord sharding into {num_shards} files...\")\n\n    # Iterate through each shard\n    for i in range(num_shards):\n        # Calculate the start and end indices for this shard\n        start_index = i * shard_size\n        end_index = min((i + 1) * shard_size, total_images)\n        \n        # Get the subset of image paths for the current shard\n        shard_img_paths = all_img_paths[start_index:end_index]\n        \n        # Format the output filename to indicate the shard number\n        # e.g., 'data-00000-of-00005.tfrecord'\n        shard_filename = f\"data-{i:05d}-of-{num_shards:05d}.tfrecord\"\n        output_path = os.path.join(output_dir, shard_filename)\n        \n        num_written = 0\n        with tf.io.TFRecordWriter(output_path) as writer:\n            for img_path in shard_img_paths:\n                try:\n                    fname = os.path.splitext(os.path.basename(img_path))[0]\n                    annot_path = os.path.join(annot_dir, fname + \".txt\")\n\n                    # Use your existing helper functions\n                    record = parse_image_and_labels(img_path, annot_path)\n                    tf_example = create_tf_example(record)\n\n                    # Write to TFRecord\n                    writer.write(tf_example.SerializeToString())\n                    num_written += 1\n                except Exception as e:\n                    print(f\"Error processing {img_path}: {e}\")\n                    \n        print(f\"✅ Shard {i + 1}/{num_shards}: {num_written} samples written to {output_path}\")\n        \n    print(\"✅ All TFRecord shards have been created.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Train set\nwrite_tfrecord(\n    image_dir=\"/kaggle/input/rdd-2022/RDD_SPLIT/train/images\",\n    annot_dir=\"/kaggle/input/rdd-2022/RDD_SPLIT/train/labels\",\n    output_dir=\"train_tfrecords\",\n    num_shards = 100\n)\n\n# Validation set\nwrite_tfrecord(\n    image_dir=\"/kaggle/input/rdd-2022/RDD_SPLIT/val/images\",\n    annot_dir=\"/kaggle/input/rdd-2022/RDD_SPLIT/val/labels\",\n    output_dir=\"val_tfrecrods\",\n    num_shards = 10\n)\n\n# Test set\nwrite_tfrecord(\n    image_dir=\"/kaggle/input/rdd-2022/RDD_SPLIT/test/images\",\n    annot_dir=\"/kaggle/input/rdd-2022/RDD_SPLIT/test/labels\",\n    output_dir=\"test_tfrecords\",\n    num_shards = 10\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_description = {\n    \"image/filename\": tf.io.FixedLenFeature([], tf.string), # Add this line\n    \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n    \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n    \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n    \"image/object/bbox/xmin\": tf.io.VarLenFeature(tf.float32),\n    \"image/object/bbox/ymin\": tf.io.VarLenFeature(tf.float32),\n    \"image/object/bbox/xmax\": tf.io.VarLenFeature(tf.float32),\n    \"image/object/bbox/ymax\": tf.io.VarLenFeature(tf.float32),\n    \"image/object/class/label\": tf.io.VarLenFeature(tf.int64)\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_BOXES = max_boxes_found\nSENTINEL_BOX = [0.0, 0.0, 0.0, 0.0]  # empty box\nBACKGROUND_LABEL = -1  # empty box label\nIMAGE_SIZE = 512\ndef parse_tfrecord(example_proto):\n    \"\"\"\n    Parses a single tf.train.Example from a TFRecord file, using a globally\n    defined `feature_description` dictionary.\n\n    Args:\n        example_proto: A serialized tf.train.Example.\n\n    Returns:\n        A tuple containing the processed image and a dictionary with\n        the normalized and padded bounding boxes, labels, and the filename.\n    \"\"\"\n    # Parse the serialized example using the globally available feature_description.\n    features = tf.io.parse_single_example(example_proto, feature_description)\n\n    # Decode image from JPEG format and convert to float32, normalizing\n    # pixel values to the range [0, 1].\n    image = tf.image.decode_jpeg(features[\"image/encoded\"], channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n\n\n    \n    # Get the filename from the parsed features.\n    filename = features[\"image/filename\"]\n\n    # Convert sparse bounding box coordinate tensors to dense tensors.\n    xmin = tf.sparse.to_dense(features[\"image/object/bbox/xmin\"])\n    ymin = tf.sparse.to_dense(features[\"image/object/bbox/ymin\"])\n    xmax = tf.sparse.to_dense(features[\"image/object/bbox/xmax\"])\n    ymax = tf.sparse.to_dense(features[\"image/object/bbox/ymax\"])\n\n    # Stack the coordinates to form a single tensor of shape [num_boxes, 4].\n    # The order is ymin, xmin, ymax, xmax.\n    boxes = tf.stack([ymin, xmin, ymax, xmax], axis=1)\n\n    # Convert sparse label tensor to a dense tensor and cast to int32.\n    # int32 is a common and efficient type for label handling in TensorFlow.\n    labels = tf.sparse.to_dense(features[\"image/object/class/label\"])\n    labels = tf.cast(labels, tf.int32)\n\n    num_boxes = tf.shape(boxes)[0]\n\n    # Pad boxes and labels to a fixed size (MAX_BOXES) using a conditional operation.\n    # This ensures that all tensors in the batch have the same shape.\n    pad_size = MAX_BOXES - num_boxes\n    \n    # If the number of boxes is less than MAX_BOXES, pad with sentinel values.\n    # If it's more, truncate the tensors to the maximum allowed.\n    boxes = tf.cond(pad_size > 0,\n                    lambda: tf.concat([boxes, tf.tile([SENTINEL_BOX], [pad_size,1])], axis=0),\n                    lambda: boxes[:MAX_BOXES])\n    labels = tf.cond(pad_size > 0,\n                     lambda: tf.concat([labels, tf.fill([pad_size], BACKGROUND_LABEL)], axis=0),\n                     lambda: labels[:MAX_BOXES])\n\n    # Return the image and a dictionary containing the processed data, including the filename.\n    return image, {\"boxes\": boxes, \"classes\": labels}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# Define the paths to your directories\ntrain_dir = '/kaggle/working/train_tfrecords'\nval_dir = '/kaggle/working/val_tfrecrods'\n\n# 1. Get the list of filenames and create full paths\ntrain_files = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\nval_files = [os.path.join(val_dir, f) for f in os.listdir(val_dir)]\n\n\n# 2. Create two separate TFRecordDataset objects\ntrain_dataset = tf.data.TFRecordDataset(train_files)\nval_dataset = tf.data.TFRecordDataset(val_files)\n\n# 3. Apply the parsing function to each dataset independently\ntrain_dataset = train_dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for img, targets in train_dataset.take(1):\n    print(\"Image shape:\", img.shape)\n    print(\"Boxes:\", targets[\"boxes\"])\n    print(\"ckasses:\", targets[\"classes\"])\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# This dictionary should be consistent with the labels used in your TFRecord creation.\nLABEL_MAP = {\n    0: 'longitudinal crack',\n    1: 'transverse crack',\n    2: 'alligator crack',\n    3: 'other corruption',\n    4: 'Pothole',\n    5: 'Background'  # Background label for padding\n}\n\ndef visualize_boxes(example):\n    \"\"\"\n    Visualizes an image from a parsed TFRecord example with bounding boxes.\n    \n    This function takes a single example from the dataset, extracts the image and\n    bounding box information, and displays the image with the boxes drawn on it.\n    It is useful for debugging and verifying that the data has been parsed\n    and normalized correctly.\n    \n    Args:\n        example: A tuple containing the image tensor and a dictionary of features.\n                 Expected format: (image_tensor, {'boxes': boxes_tensor, 'labels': labels_tensor, 'filename': filename_tensor})\n    \"\"\"\n    # Unpack the example\n    image, features = example\n    boxes = features['boxes']\n    labels = features['classes']\n\n    # Convert tensors to NumPy arrays\n    # The image is normalized to [0, 1] in the parsing function, so we scale it back to [0, 255]\n    image_np = (image.numpy() * 255).astype(np.uint8)\n    boxes_np = boxes.numpy()\n    labels_np = labels.numpy()\n    \n    # Get image dimensions to convert normalized box coordinates to pixel coordinates\n    height, width, _ = image_np.shape\n\n    # Create a figure and axes\n    fig, ax = plt.subplots(1)\n    \n    # Display the image\n    ax.imshow(image_np)\n\n\n\n\n    # Iterate through each bounding box and draw it\n    for i, box in enumerate(boxes_np):\n        label_id = labels_np[i]\n\n        # Filter out the padded boxes where the label is the background label (5)\n        if label_id != 5:\n            ymin, xmin, ymax, xmax = box\n\n            # Convert normalized coordinates to pixel coordinates\n            box_xmin = xmin * width\n            box_ymin = ymin * height\n            box_width = (xmax - xmin) * width\n            box_height = (ymax - ymin) * height\n\n            # Create a Rectangle patch\n            rect = patches.Rectangle(\n                (box_xmin, box_ymin), \n                box_width, \n                box_height, \n                linewidth=2, \n                edgecolor='r', \n                facecolor='none'\n            )\n\n            # Add the patch to the Axes\n            ax.add_patch(rect)\n            \n            # Add a text label above the box\n            label_text = LABEL_MAP.get(label_id)\n            plt.text(box_xmin, box_ymin - 5, label_text, \n                     color='white', \n                     bbox=dict(facecolor='red', alpha=0.8),\n                     fontsize=8)\n\n    # Display the plot\n    plt.show()\n\n# To use these functions, you would first create a TFRecordDataset,\n# then map the `parse_tfrecord_example` function over it, and finally\n# pass individual examples to the `visualize_boxes` function.\n# Example:\n# train_dataset = tf.data.TFRecordDataset(your_tfrecord_files)\n# parsed_dataset = train_dataset.map(parse_tfrecord_example)\n# for example in parsed_dataset.take(5):\n#     visualize_boxes(example)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for example in train_dataset.take(1):\n    visualize_boxes(example)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\ndef flip_boxes_left_right(boxes):\n    ymin, xmin, ymax, xmax = tf.unstack(boxes, axis=1)\n    xmin_new = 1.0 - xmax\n    xmax_new = 1.0 - xmin\n    return tf.stack([ymin, xmin_new, ymax, xmax_new], axis=1)\n\ndef rotate_boxes_90(boxes):\n    ymin, xmin, ymax, xmax = tf.unstack(boxes, axis=1)\n    new_ymin = 1.0 - xmax\n    new_xmin = ymin\n    new_ymax = 1.0 - xmin\n    new_xmax = ymax\n    return tf.stack([new_ymin, new_xmin, new_ymax, new_xmax], axis=1)\n\n\ndef augment_image(image, boxes, classes):\n    # build mask for valid boxes (not padding)\n    valid_mask = tf.expand_dims(tf.not_equal(classes, -1), -1)\n    valid_mask = tf.tile(valid_mask, [1, 4])\n\n    # horizontal flip\n    if tf.random.uniform([]) > 0.5:\n        image = tf.image.flip_left_right(image)\n        boxes = tf.where(valid_mask, flip_boxes_left_right(boxes), boxes)\n\n    # brightness & contrast\n    image = tf.image.random_brightness(image, max_delta=0.1)\n    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n\n    # 90° rotation\n    if tf.random.uniform([]) > 0.7:\n        image = tf.image.rot90(image)\n        boxes = tf.where(valid_mask, rotate_boxes_90(boxes), boxes)\n\n    image = tf.clip_by_value(image, 0., 1.)\n    return image, boxes, classes\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_fn(image, targets):\n    boxes = targets[\"boxes\"]\n    labels = targets[\"classes\"]\n\n    image, boxes, labels = augment_image(image, boxes, labels)\n\n    return image, {\"boxes\": boxes, \"classes\": labels}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 64  # small for CPU debug\n# parsed dataset from Step 1\naugmented_dataset = train_dataset.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Add the denormalization step\n# This applies the denormalization to each sample individually\naugmented_dataset = augmented_dataset.map(lambda img, boxes: (\n    tf.cast(img * 255.0, tf.float32), \n    {\n        'boxes': boxes['boxes'] * tf.cast(tf.stack([IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE]), tf.float32), \n        'classes': boxes['classes']\n    }\n))\n\n# 3. Continue with batching and prefetching\naugmented_dataset = augmented_dataset.batch(BATCH_SIZE, drop_remainder=True)\naugmented_dataset = augmented_dataset.prefetch(tf.data.AUTOTUNE)\n\n\nval_dataset = val_dataset.map(lambda img, boxes: (\n    tf.cast(img * 255.0, tf.float32), \n    {\n        'boxes': boxes['boxes'] * tf.cast(tf.stack([IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE]), tf.float32), \n        'classes': boxes['classes']\n    }\n))\n\n# 3. Continue with batching and prefetching\nval_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\nval_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for batch_images, batch_targets in augmented_dataset.take(5):\n    print(\"Batch images shape:\", batch_images.shape, batch_images.dtype)\n    print(\"Batch boxes shape:\", batch_targets[\"boxes\"].shape, batch_targets[\"boxes\"].dtype)\n    print(\"Batch labels shape:\", batch_targets[\"classes\"].shape, batch_targets[\"classes\"].dtype)\n\n    # Optional: visualize first image and boxes\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n\n    img = batch_images[0].numpy()\n    boxes = batch_targets[\"boxes\"][0].numpy()\n    \n    fig, ax = plt.subplots(1,1)\n    ax.imshow(img)\n    for box in boxes:\n        if (box != [0,0,0,0]).any():  # ignore sentinel boxes\n            ymin, xmin, ymax, xmax = box\n            rect = patches.Rectangle(\n                (xmin*img.shape[1], ymin*img.shape[0]),\n                (xmax - xmin)*img.shape[1],\n                (ymax - ymin)*img.shape[0],\n                linewidth=2, edgecolor='r', facecolor='none'\n            )\n            ax.add_patch(rect)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for images, targets in augmented_dataset.take(1):\n    print(\"image range:\", tf.reduce_min(images), tf.reduce_max(images))\n    print(\"boxes example:\", targets[\"boxes\"][0][:5])\n    print(\"classes example:\", targets[\"classes\"][0][:5])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# TPU strategy (optional)\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect('local')\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(\"✅ TPU detected\")\nexcept:\n    strategy = tf.distribute.get_strategy()\n    print(\"✅ Using\", strategy)\n\nIMG_SIZE = 512\nNUM_CLASSES = 5   # only foreground classes, background handled automatically\nBATCH_SIZE = 64\nEPOCHS = 100\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras_cv.models import RetinaNet, ResNet50Backbone\nwith strategy.scope():\n    backbone = ResNet50Backbone(include_rescaling=True)\n    model = RetinaNet(\n        backbone=backbone,\n        include_rescaling=True,          # we already resized\n        bounding_box_format=\"yxyx\",\n        num_classes=NUM_CLASSES,\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-4),\n        classification_loss=\"focal\",\n        box_loss=\"SmoothL1\"\n    )\n    model.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    augmented_dataset,\n    validation_data=val_dataset,\n    epochs=50,\n    callbacks=[\n        tf.keras.callbacks.ModelCheckpoint(\"retinanet_best.h5\",\n                                           save_best_only=True,\n                                           monitor=\"val_loss\"),\n        tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n    ]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model2 = tf.keras.models.load_model('retinanet_best.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model as a SavedModel\nmodel2.save('retinanet_best_savedmodel', save_format='tf')\n\nprint(\"Model converted successfully to SavedModel format at Model/retinanet_best_savedmodel\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}